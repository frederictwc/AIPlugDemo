{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 6 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:batch_all_reduce: 8 all-reduces with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in converted code:\n\n    <ipython-input-2-27bf35756c77>:215 train_step  *\n        g_mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.MEAN, g_per_example_losses, axis=0)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:852 reduce\n        denom = self._extended._reduce(reduce_util.ReduceOp.SUM, denom)  # pylint: disable=protected-access\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1436 _reduce\n        device_util.current() or \"/device:CPU:0\"))[0]\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py:701 _reduce_to\n        reduce_op, self._device_map, value, destinations)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py:102 reduce_non_distributed_value\n        \"the given reduce op %s.\" % (value, reduce_op))\n\n    ValueError: A non-DistributedValues value 128 cannot be reduced with the given reduce op ReduceOp.SUM.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-27bf35756c77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdist_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;31m#cv2.imwrite( str(j)+'.jpg', X[0, :, :, 0]*255)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0;31m#if(j%500==0):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0;31m#       X_re, _ = generate_fake_samples(g, 100, 64)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0minitializer_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject_identity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectIdentityDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    368\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    369\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 370\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m         \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgraph_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2147\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2148\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2036\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2037\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2038\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2039\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2040\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    913\u001b[0m                                           converted_func)\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    903\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in converted code:\n\n    <ipython-input-2-27bf35756c77>:215 train_step  *\n        g_mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.MEAN, g_per_example_losses, axis=0)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:852 reduce\n        denom = self._extended._reduce(reduce_util.ReduceOp.SUM, denom)  # pylint: disable=protected-access\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/distribute_lib.py:1436 _reduce\n        device_util.current() or \"/device:CPU:0\"))[0]\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/mirrored_strategy.py:701 _reduce_to\n        reduce_op, self._device_map, value, destinations)\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/distribute/cross_device_ops.py:102 reduce_non_distributed_value\n        \"the given reduce op %s.\" % (value, reduce_op))\n\n    ValueError: A non-DistributedValues value 128 cannot be reduced with the given reduce op ReduceOp.SUM.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "#import cv2\n",
    "from numpy import expand_dims\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "cuda_count = 3\n",
    "global_batch_size = 128*cuda_count\n",
    "\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tx_input = np.random.uniform(0.0, 1.0, latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tx_input = x_input.reshape(n_samples, latent_dim)\n",
    "\treturn x_input\n",
    "\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tx_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\tX = g_model.predict(x_input)\n",
    "\t# create 'fake' class labels (0)\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn X, y\n",
    "\n",
    "def discriminator(inshape=(28,28,1)):\n",
    "\tinputs = tf.keras.Input(shape=inshape)\n",
    "\tx = tf.keras.layers.Conv2D(64,(3,3),strides=(2,2),padding='same', activation = tf.nn.leaky_relu)(inputs)\n",
    "\t#x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\tx = tf.keras.layers.Dropout(0.4)(x)\n",
    "\tx = tf.keras.layers.Conv2D(64,(3,3),strides=(2,2),padding='same', activation = tf.nn.leaky_relu)(x)\n",
    "\t#x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\tx = tf.keras.layers.Dropout(0.4)(x)\n",
    "\tx = tf.keras.layers.Flatten()(x)\n",
    "\toutputs = tf.keras.layers.Dense(100, activation = 'sigmoid')(x)\n",
    "\t#outputs = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\td = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "\treturn d\n",
    "\n",
    "def generator(latent_dim):\n",
    "\tg_inputs = tf.keras.Input(shape=(latent_dim,))\n",
    "\tx = tf.keras.layers.Dense(128*7*7, activation = tf.nn.leaky_relu)(g_inputs)\n",
    "\t#x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\tx = tf.keras.layers.Reshape( (7,7,128), input_shape=(128*7*7,))(x)\n",
    "\tx = tf.keras.layers.Conv2DTranspose(128,(4,4),strides=(2,2), padding='same', activation = tf.nn.leaky_relu)(x)\n",
    "\t#x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\tx = tf.keras.layers.Conv2DTranspose(128,(4,4),strides=(2,2), padding='same', activation = tf.nn.leaky_relu)(x)\n",
    "\t#x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\tg_outputs = tf.keras.layers.Conv2D(1,(7,7), activation='sigmoid',padding='same')(x)\n",
    "\tg = tf.keras.Model(inputs=g_inputs,outputs=g_outputs)\n",
    "\treturn g\n",
    "\n",
    "train, test = tf.keras.datasets.mnist.load_data()\n",
    "X, L = train\n",
    "X = X.astype('float32')\n",
    "X_use = X / 255.0\n",
    "X_use = np.expand_dims(X_use, axis=-1)\n",
    "#control = np.array([0,0,0,0,0,1])\n",
    "#Y = np.tile(control,int(X.shape[0]/6.0/global_batch_size))\n",
    "#print(X[0])\n",
    "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with mirrored_strategy.scope():\n",
    "\n",
    "\t@tf.function\n",
    "\tdef train_step(X ,l, epsilon): \n",
    "\t\t'''\n",
    "\t\tdef step_1(X, latent_dim, epsilon):\n",
    "\n",
    "\t\t\tscale = 10.0\n",
    "\t\t\tsize = X.shape[0]\n",
    "\t\t\tx_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\tX_fake1 = g(x_input1)\n",
    "\t\t\tyy2 = d(X_fake1)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\twith tf.GradientTape() as tape1:\n",
    "\t\t\t\t\n",
    "\t\t\t\tdef safe_norm(x, e=1e-15, axis=None):\n",
    "\t\t\t\t\treturn tf.sqrt(tf.reduce_sum( tf.square(x), axis=axis) + e)\n",
    "\n",
    "\t\t\t\ttape1.watch(d.trainable_variables)\n",
    "\t\t\t\tx_input2 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\tyy2_pre = g(x_input2)\n",
    "\t\t\t\tyy1 = d(yy2_pre)\n",
    "\t\t\t\txx1 = d(X)\n",
    "\t\t\t\t\n",
    "\t\t\t\td_loss_pre  = (safe_norm(yy1 - yy2, axis = 1) - safe_norm(yy1, axis = 1)) - (safe_norm(xx1 - yy2, axis = 1) - safe_norm(xx1, axis = 1))\n",
    "\t\t\t\t\n",
    "\t\t\t\tx_hat = epsilon * xx1 + (1 - epsilon) * yy1\n",
    "\n",
    "\t\t\t\tddx_pre = tf.gradients(safe_norm(x_hat - yy2, axis = 1) - safe_norm(x_hat, axis = 1), x_hat)[0]\n",
    "\t\t\t\t\n",
    "\t\t\t\tddx = tf.square(safe_norm(ddx_pre, axis=1) - 1.0) * scale\n",
    "\n",
    "\n",
    "\t\t\t\td_loss = (d_loss_pre + ddx)*(1/size)\n",
    "\n",
    "\t\t\t\t#d_loss = d_loss_pre \n",
    "\t\t\t\t\n",
    "\t\t\tgrads_d = tape1.gradient(d_loss, d.trainable_variables)\n",
    "\t\t\topt1.apply_gradients(list(zip(grads_d, d.trainable_variables)))\n",
    "\n",
    "\t\t\treturn d_loss\n",
    "\n",
    "\t\tdef step_2(X, latent_dim):\n",
    "\n",
    "\t\t\tsize = X.shape[0]\t\n",
    "\t\t\txx1 = d(X)\n",
    "\t\t\tx_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\tyy2_pre = g(x_input1)\n",
    "\t\t\tyy2 = d(yy2_pre)\n",
    "\t\t\t\n",
    "\n",
    "\t\t\twith tf.GradientTape() as tape2:\n",
    "\t\t\t\tdef safe_norm(x, e=1e-15, axis=None):\n",
    "\t\t\t\t\treturn tf.sqrt(tf.reduce_sum( tf.square(x) , axis=axis) + e)\n",
    "\n",
    "\t\t\t\ttape2.watch(g.trainable_variables)\n",
    "\t\t\t\tx_input2 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\tyy1_pre = g(x_input2)\n",
    "\t\t\t\tyy1 = d(yy1_pre)\n",
    "\t\t\t\tg_loss =  (safe_norm(xx1 - yy2, axis = 1) - safe_norm(xx1, axis = 1)) - (safe_norm(yy1 - yy2, axis = 1) - safe_norm(yy1, axis = 1))*(1/size) \n",
    "\t\t\t\n",
    "\t\t\tgrads_g = tape2.gradient(g_loss, g.trainable_variables)\n",
    "\t\t\t#print(grads_g)\n",
    "\t\t\topt2.apply_gradients(list(zip(grads_g, g.trainable_variables)))\n",
    "\t\t\treturn g_loss\n",
    "\t\t'''\t\t\n",
    "\t\tdef step_fn(X, latent_dim, epsilon):\n",
    "\n",
    "\t\t\tx = tf.split(X, num_or_size_splits=6, axis=0)\n",
    "\t\t\tscale = 10.0\n",
    "\t\t\tsize = x[0].shape[0]\n",
    "\n",
    "\t\t\tfor i in range(5):\n",
    "\n",
    "\t\t\t\t#x_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\t#X_fake1 = g(x_input1)\n",
    "\t\t\t\t#yy2 = d(X_fake1)\n",
    "\n",
    "\t\t\t\twith tf.GradientTape(persistent=True) as tape1:\n",
    "\t\t\t\t\tdef safe_norm(x, e=1e-15, axis=None):\n",
    "\t\t\t\t\t\treturn tf.sqrt(tf.reduce_sum( tf.square(x), axis=axis) + e)\n",
    "\n",
    "\t\t\t\t\t#tape1.watch(d.trainable_variables)\n",
    "\t\t\t\t\tx_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\t\tX_fake1 = g(x_input1)\n",
    "\t\t\t\t\tyy2 = d(X_fake1, training = False)\n",
    "\t\t\t\t\tx_input2 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\t\tyy2_pre = g(x_input2)\n",
    "\t\t\t\t\tyy1 = d(yy2_pre)\n",
    "\t\t\t\t\txx1 = d(x[i])\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\td_loss_pre  = (tf.norm(yy1 - yy2, axis = 1) - tf.norm(yy1, axis = 1)) - (tf.norm(xx1 - yy2, axis = 1) - tf.norm(xx1, axis = 1))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tx_hat = epsilon * xx1 + (1 - epsilon) * yy1\n",
    "\n",
    "\t\t\t\t\twith tf.GradientTape(persistent=True) as itape:\n",
    "\t\t\t\t\t\titape.watch(x_hat)\n",
    "\t\t\t\t\t\tddx_pre = (tf.norm(x_hat - yy2, axis = 1) - tf.norm(x_hat, axis = 1))\n",
    "\t\t\t\t\t#\tddx_pre = x_hat*x_hat\n",
    "\t\t\t\t\tddx_pre2 = itape.gradient(ddx_pre, x_hat)\n",
    "\t\t\t\t\t#ddx_pre = tf.gradients((tf.norm(x_hat - yy2, axis = 1) - tf.norm(x_hat, axis = 1)) , x_hat)[0]\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\tddx = tf.square(tf.norm(ddx_pre2, axis=1) - 1.0) * scale\n",
    "\n",
    "\n",
    "\t\t\t\t\td_loss = (d_loss_pre + ddx)\n",
    "\n",
    "\t\t\t\t\t#d_loss = d_loss_pre \n",
    "\t\t\t\t\t\n",
    "\t\t\t\tgrads_d = tape1.gradient(d_loss, d.trainable_variables)\n",
    "\t\t\t\td_op = opt1.apply_gradients(list(zip(grads_d, d.trainable_variables)))\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t#xx1 = d(x[5])\n",
    "\t\t\t#x_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t#yy2_pre = g(x_input1)\n",
    "\t\t\t#yy2 = d(yy2_pre)\n",
    "\n",
    "\t\t\twith tf.GradientTape() as tape2:\n",
    "\t\t\t\tdef safe_norm(x, e=1e-15, axis=None):\n",
    "\t\t\t\t\treturn tf.sqrt(tf.reduce_sum( tf.square(x) , axis=axis) + e)\n",
    "\n",
    "\t\t\t\t#tape2.watch(g.trainable_variables)\t\n",
    "\t\t\t\txx1 = d(x[5])\n",
    "\t\t\t\tx_input1 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\tyy2_pre = g(x_input1, training = False)\n",
    "\t\t\t\tyy2 = d(yy2_pre)\n",
    "\n",
    "\t\t\t\tx_input2 = tf.random.uniform(shape=[size, latent_dim],minval=0.0,maxval=1.0)\n",
    "\t\t\t\tyy1_pre = g(x_input2)\n",
    "\t\t\t\tyy1 = d(yy1_pre)\n",
    "\t\t\t\tg_loss =  ((tf.norm(xx1 - yy2, axis = 1) - tf.norm(xx1, axis = 1)) - (tf.norm(yy1 - yy2, axis = 1) - tf.norm(yy1, axis = 1))) \n",
    "\t\t\t\n",
    "\t\t\tgrads_g = tape2.gradient(g_loss, g.trainable_variables)\n",
    "\t\t\tg_op = opt2.apply_gradients(list(zip(grads_g, g.trainable_variables)))\n",
    "\n",
    "\t\t\treturn g_loss\n",
    "\n",
    "\t\t#x = tf.split(X, num_or_size_splits=6, axis=0)\n",
    "\n",
    "\t\t'''\n",
    "\t\tfor i in range(5):\n",
    "\t\t\td_per_example_losses = mirrored_strategy.experimental_run_v2(step_1, args=(x[i], l, epsilon))\n",
    "\t\t\td_mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.MEAN, d_per_example_losses, axis=0)\n",
    "\n",
    "\t\tg_per_example_losses = mirrored_strategy.experimental_run_v2(step_2, args=(x[5], l))\n",
    "\t\tg_mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.MEAN, g_per_example_losses, axis=0)\n",
    "\t\t'''\n",
    "\t\tg_per_example_losses = mirrored_strategy.experimental_run_v2(step_fn, args=(X, l, epsilon))\n",
    "\t\tg_mean_loss = mirrored_strategy.reduce(tf.distribute.ReduceOp.MEAN, g_per_example_losses, axis=0)\n",
    "\n",
    "\t\t#for var in d.trainable_variables:\n",
    "\t\t#\t\tvar.assign(tf.clip_by_value(var, -0.01, 0.01))\n",
    "\t\t#for var in gan_model.trainable_variables:\n",
    "\t\t\t\t#var.assign(tf.clip_by_value(var, -0.1, 0.1))\n",
    "\t\t#d.trainable = False\n",
    "\t\t\n",
    "\t\treturn g_mean_loss\n",
    "\n",
    "\td = discriminator()\n",
    "\tg = generator(100)\n",
    "\tepsilon = tf.random.uniform([], 0.0, 1.0)\n",
    "\t#train_acc = tf.keras.metrics.BinaryAccuracy()\n",
    "\t\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices( X_use ).batch(int(global_batch_size*6)).shuffle(1000).repeat(2400)\n",
    "\tdist_dataset = mirrored_strategy.experimental_distribute_dataset(dataset)\n",
    "\topt1 = tf.keras.optimizers.Adam(lr=2e-4, beta_1=0.5, beta_2=0.9)\n",
    "\topt2 = tf.keras.optimizers.Adam(lr=2e-4, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\t#o1 = tf.compat.v1.train.AdamOptimizer.minimize(op1,lr=2e-4, beta_1=0.5, beta_2=0.9)\n",
    "\t#o2 = tf.compat.v1.train.AdamOptimizer.minimize(op2,lr=2e-4, beta_1=0.5, beta_2=0.9)\n",
    "\n",
    "\t#g.trainable = False\t\n",
    "\t#j = 0\n",
    "\tfor inputs in dist_dataset:\n",
    "\t\t#cv2.imwrite( str(j)+'.jpg', X[0, :, :, 0]*255)\n",
    "\t\tprint(train_step(inputs ,100, epsilon))\n",
    "\t\t#if(j%500==0):\n",
    "\t\t#\tX_re, _ = generate_fake_samples(g, 100, 64)\n",
    "\t\t#\tdd = np.reshape(X_re[:,:,:,0], [8*X_re.shape[1], 8*X_re.shape[1]] )\n",
    "\t\t#\tcv2.imwrite( str(j) +'.jpg', dd*255.0 )\n",
    "\t\t#j = j+1\n",
    "\t\t\n",
    "n_samples = 25\n",
    "X, _ = generate_fake_samples(g, 100, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-rc0\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -c 'import tensorflow as tf; print(tf.__version__)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
